
\subsection{Unconstrained Markov Decision Processes} \label{sec:unconstrained_mds}
\begin{definition}
    (\textbf{Markov Decision Process})
    We define a \textbf{Markov Decision Process} (MDP) as the following $6$-tuple: 
    \begin{align*}
        \textestimated{M} = (S,A,P,\bm{\nu},\bm{r},\gamma),
    \end{align*}
    where $S = \{s_1,s_2,...,s_n\}$ is a discrete set of \textbf{states} of size $|S| = n$, $A = \{a_1,a_2,...,a_m\}$ a discrete set of \textbf{actions} of size $|A|=m$, is a probabilistic markovian transition law $P:S\times A \rightarrow \Delta_{S}$ that we can represent as a $\mathbb{R}^{nm\times n}$ matrix (where each column represents in the distribution $\in \Delta_S$), $\nu\in \Delta_s$ is the \textbf{initial distribution} of states at the start of the process, $r:S \times A\rightarrow \mathbb{R}$ is a reward function that we can conveniently represent as a vector $\bm{r}\in\mathcal{R}\subseteq\mathbb{R}^{nm}$ since $S$ and $A$ are discrete (we call $\mathcal{R}$ the \textbf{reward class}), finally $\gamma\in(0,1)$ is our \textbf{discount factor}. \\
\end{definition}


Unless specified otherwise it is expected that Markov Decision Process run for an infinite time, each (infinite-time) sequence of move on the MDP is called a \textit{trajectory} and is denoted:
\begin{align*}
    \tau = \{s_1,a_1,s_2,a_2,...\},
\end{align*}
\noindent
where the transition from one state to another is governed by the markovian transition law: 
\begin{align*}
    s_{t+1} \sim P(\cdot|s_t,a_t).
\end{align*} 
The MDP, doesn't specify how actions are selected, in practice, this is done by choosing a \textit{policy function} $\pi:S\rightarrow \Delta_A$, which we use to pick the action $a_t$:
\begin{align*}
    a_t \sim \pi(s_t) \in \Delta_A.
\end{align*}

\begin{definition}
    (\textbf{Policy})
    A \textbf{policy} is a function $\pi:S \rightarrow \Delta_A$ that maps each discrete state to a distribution over actions of an MDP $\textit{M}$. Because we consider discrete state and action sets, we can represent policies as matrices $\pi\in \Pi \in \mathbb{R}^{n \times m}$. Where $\Pi$ denotes the set of \textbf{valid policies}, i.e. : 
    \begin{align*}
        \Pi := \Big\{ \pi = [\pi_{s_1},\pi_{s_2},...,\pi_{s_n}], \pi_{s_i}\in \Delta_A \forall s_i \in S \Big\} .
    \end{align*}
\end{definition}
\noindent
Assuming actions are picked from some fixed policy function $\bm{\pi}\Pi$ the progression from state to state of the MDP becomes a Markov Chain described by the closed-loop transition law.

\begin{definition}
    \label{def:closed_loop_transition_law}
    (\textbf{Closed-Loop Transition Law})
    We define the \textbf{closed-loop transition law} $P^\pi :S \rightarrow \Delta_S$ as a function that gives the distribution over the next state as a function of the current state, this distribution is trivially given by: 
    \begin{align*}
        P^\pi(s'|s) = \sum_a P(s'|a,s) \cdot \pi(a|s).
    \end{align*}
    \noindent
    It can be represented as a square matrix $P^\pi \in \mathbb{R}^{n \times n}$, where each $i$-th column gives the distribution of states $s'$ associated with the transition from the $i$-th state.
\end{definition}

\subsubsection{Discounted Reward, Value, Quality and Objective Functions}

At each transition the agent receives reward $r_t = r(s_t,a_t)$. We want to quantify the general performance of our agents with respect to that reward function, across complete trajectories. This motivates the definition a series of functions which are used to quantify the performance of a given policy $\pi$ on the MDP. 

\begin{definition}
    (\textbf{Discounted Reward})
    Given a policy $\pi$, and a convex \textbf{policy regularizer function} $\Omega: \Pi \rightarrow \mathbb{R}$, one can compute the \textit{discounted reward} $R(\tau)$ as follows:
    \begin{align*}
        R(\tau) = (1-\gamma) \lim_{T \rightarrow + \infty}\sum_{t=0}^{T} \gamma^t \Bigl( r(s_t,a_t) - \Omega(\pi(\cdot|s_t)) \Bigr) ;~ s_t,a_t \in \tau.
    \end{align*}
    Note that assuming that the regularized rewards ($r(s_t,a_t)- \Omega(\pi(\cdot|s_t))$) are upper-bounded by some constant $C\in \mathbb{R}$, this limit is guaranteed to exist as:
    \begin{align*}
        R(\tau) = (1-\gamma) \lim_{T \rightarrow + \infty}\sum_{t=0}^{T} \gamma^t \Big(r(s_t,a_t)- \Omega(\pi(\cdot|s_t))\Big) \leq (1-\gamma) \lim_{T \rightarrow + \infty}\sum_{t=0}^{T} \gamma^t C = C.
    \end{align*}
\end{definition}
\noindent
Since the process is stochastic it makes more sense to think in terms of \textit{expected discounted reward}, we define the \textit{objective function} $J(\bm{\pi},\bm{r})$, the \textit{value function} $V^\pi_r(s)$ and the \textit{quality function} $Q^\pi_r(s,a)$ (which are all associated with a policy $\pi$ and a reward function $r$).

\begin{definition}
    (\textbf{Value Function})
    For a given policy $\pi:S\rightarrow \Delta A$, a convex \textbf{policy regularizer function} $\Omega: \Pi \rightarrow \mathbb{R}$ and under the assumption that the agent starts at some state $s$, we define the \textbf{value function} $V^\pi_r:S\rightarrow \mathbb{R}$ as:
    \begin{align*}
        V_r^\pi(s) &= \mathbb{E}\Big[ R(\tau) \Big| s_0  = s, \pi \Big] \\
         &= \mathbb{E}\Bigg[ (1-\gamma) \sum_{t=0}^{+\infty} \gamma^t \Bigl(  r(s_t,a_t) - \Omega(\pi(\cdot|s_t))  \Bigr) \Big| s_0  = s, \pi \Bigg].
    \end{align*}
\end{definition}

\begin{definition}
    (\textbf{Quality Function})
    For a given policy $\pi:S\rightarrow \Delta A$, a convex \textbf{policy regularizer function} $\Omega: \Pi \rightarrow \mathbb{R}$ and under the assumption that the agent starts at some state $s$ with some first action $a$, we define the \textbf{quality function} $Q_r^\pi:S\times A \rightarrow \mathbb{R}$ as:
    \begin{align*}
        Q_r^\pi(s,a) &= \mathbb{E}\Big[ R(\tau) \Big| s_0  = s, a_0  = a, \pi \Big] \\
        \\ &= \mathbb{E}\Bigg[ (1-\gamma) \sum_{t=0}^{+\infty} \gamma^t \Bigl( r(s_t,a_t) - \Omega(\pi(\cdot|s_t))  \Bigr)  \Big| s_0  = s, a_0 = a, \pi \Bigg].
    \end{align*}
\end{definition}

\begin{definition}
    \label{def:obj_function}
    (\textbf{Objective Function})
    For a given policy $\pi:S\rightarrow \Delta A$, convex policy regularizer function $\Omega: \Pi \rightarrow \mathbb{R}$ and and under an initial state distribution $\bm{\nu} \in \Delta_S$ we define the \textbf{objective function} as:
    \begin{align*}
        J(\pi,r) = \mathbb{E}\Big[ R(\tau) \Big| s_0 \sim \nu, \pi \Big] =  \mathbb{E}\Big[ V^\pi(s) \Big| s \sim \nu\Big].
    \end{align*}
    Since we most often use the \textbf{objective function} for optimization in policy space, we consider that $J:\Pi \times \mathcal{R} \rightarrow \mathbb{R}$ is a function of the policy $\pi$ and the reward $r$.
\end{definition}

\subsubsection{The Occupancy Measure}
\label{sec:occupancy_measure}
We now discuss a descriptor of the distribution of an agent's position across all states induced by some policy $\pi$: the \textit{occupancy measure} is defined as follows.

\begin{definition}
    (\textbf{Occupancy Measure}) 
    The (state-action) \textbf{occupancy measure} $\bm{\mu}^\pi \in \Delta_{S \times A}$ is defined for a given state-action pair as follows:
    \begin{align*}
        \mu^\pi(s,a) &= (1-\gamma) \lim_{T \rightarrow + \infty} \sum_{t=0}^T \gamma^t \mathbb{P}_\nu^\pi(s_t=s,a_t=a) 
        \\
        &= (1-\gamma) \mathbb{E} \Big[ \sum_{t=0}^{+\infty} \gamma^t \mathbf{1}(s_t=s,a_t=a) \Big| s_0 \sim \nu, \pi\Big],
    \end{align*}
    since our state and action spaces are discrete we can represent $\bm{\mu}^{\pi}$ as a vector in $\mathbb{R}^{nm}$. 
\end{definition}

\begin{definition}
    \label{def:occupancy_set}
    (\textbf{Occupancy Set})
    The \textbf{set of valid occupancy measure} (or occupancy set) is given by:
    \begin{align*}
        \mathcal{M} := \Bigl\{ \mu \in \mathbb{R}_+^{nm} : (E-\gamma P)^T \bm{\mu} = (1-\gamma) \bm{\nu} \Bigr\}.
    \end{align*}
    The equality that all points $\bm{\mu}$ in the set must satisfy are known as \textbf{Bellman Flow Constraints}.
\end{definition}

\begin{definition}
    \label{def:state_occupancy_measure}
    (\textbf{State-Occupancy Measure}) 
    Similarly to the (state-action) occupancy measure, one can define a \textbf{state-occupancy measure} $\bm{\mu}^\pi_S \in \Delta_{S}$ is defined for a given state as follows:
    \begin{align*}
        \mu^\pi_S(s) &= (1-\gamma) \lim_{T \rightarrow + \infty} \sum_{t=0}^T \gamma^t \mathbb{P}_\nu^\pi(s_t=s) \\
        &= (1-\gamma)  \mathbb{E} \Big[ \sum_{t=0}^{+\infty} \gamma^t \mathbf{1}(s_t=s) \Big| s_0 \sim \nu, \pi\Big],
    \end{align*}
    since our state and action spaces are discrete we can represent $\bm{\mu}_S$ as a vector in $\mathbb{R}^{n}$.
\end{definition}
\noindent
Furthermore, when an state action pair $s,a$ has occupancy measure $\mu^\pi(s,a)=0$ (under policy $\pi$), we say that it is \textit{unvisited}. 

\begin{observation}
    \label{obs:mu_pi_objective}
    The occupancy measure provides us with a very concise notation for writing out quantities such as the \textbf{objective function}, which can be represented in a simple scalar product form:
    \begin{align*}
        J(\bm{\pi},\bm{r}) = \langle \bm{r}, \bm{\mu}^\pi \rangle - \tilde{\Omega}(\bm{\mu}^\pi).
    \end{align*}
    Which can be easily verified as follows:
    \begin{align*}
        J^\nu(\pi) &= (1-\gamma)  \mathbb{E}\Big[ R(\tau) \Big| s_0 \sim \nu, \pi \Big] \\
                &=  (1-\gamma) \mathbb{E}\Bigg[ \sum_{t=0}^{+\infty} \gamma^t (r(s,a) - \Omega(\pi(\cdot|s))) \Big| s_0 \sim \nu, \pi \Bigg]\\
                &= (1-\gamma)  \mathbb{E}\Bigg[ \sum_{s,a} \sum_{t=0}^{+\infty} \gamma^t  r(s,a) \textbf{1}(s,a) \Big| s_0 \sim \nu, \pi \Bigg] \\& -  (1-\gamma)  \mathbb{E}\Bigg[ \sum_{s} \sum_{t=0}^{+\infty} \gamma^t \Omega(\pi(\cdot|s))) \textbf{1}(s) \Big| s_0 \sim \nu, \pi \Bigg]   \\
                &= \sum_{s,a} r(s,a)\overbrace{ (1-\gamma) \mathbb{E}\Bigg[  \sum_{t=0}^{+\infty}\gamma^t  \textbf{1}(s,a) \Big| s_0 \sim \nu, \pi \Bigg]}^{=\bm{\mu}^\pi(s,a)}\\
                &- \sum_{s} \Omega(\pi(\cdot|s)) \overbrace{ (1-\gamma) \mathbb{E}\Bigg[  \sum_{t=0}^{+\infty} \gamma^t \textbf{1}(s) \Big| s_0 \sim \nu, \pi \Bigg]}^{=\bm{\mu}_S^\pi(s)}\\
                &= \sum_{s,a} r(s,a) \bm{\mu}^\pi(s,a) - \overbrace{\sum_{s} \Omega(\pi(\cdot|s)) \bm{\mu}_S^\pi(s) }^{:=\tilde{\Omega}(\bm{\mu})}\\
                &=  \bm{r}^T \bm{\bm{\mu}^\pi} - \tilde{\Omega}(\bm{\bm{\mu}}^\pi).
    \end{align*}
\end{observation}

\begin{lemma}
    \label{lem:convex_reg_in_om} (\textbf{Expected-Regularizer is convex in $\mu$}) Consider the function $\tilde{\Omega}:=\mathbb{E}_{s \sim \bm{\mu}}[\Omega(\pi(\cdot|s))]$,
    \begin{itemize}
        \item if $\Omega$ is \textbf{convex}, then so is $\tilde{\Omega}$
        \item if $\Omega$ is \textbf{strictly-convex}, then so is $\tilde{\Omega}$.
    \end{itemize}
    (Proof in \cite{Schlaginhaufen2023}.)
\end{lemma}

Observation \ref{obs:mu_pi_objective}, together with lemma \ref{lem:convex_reg_in_om} is quite important as it shows that the objective function is \textbf{concave} w.r.t the occupancy measure (which opens the door for optimization approaches), moreover in the special case where $\Omega(\pi(\cdot|s)) = 0, ~\forall s \in S $ the objective is \textbf{linear} w.r.t the occupancy measure (which opens the door for linear programming approaches). \\
For convenience we interchangeably write the objective as a function of the policy $\pi$ and of the occupancy measure $\bm{\mu}$:
\begin{align*}
    J^\nu(\pi) =  J^\nu(\bm{\mu}).
\end{align*} \\

\begin{observation}
    \label{obs:policy_occupancy_bijection}
    There exists a (almost) well-defined bijection between any policy $\pi$ and it's associated occupancy measure $\bm{\mu}^\pi$. With knowledge of $P$ and $\pi$, one can compute the occupancy measure from the bellman flow constraint as follows:
    \begin{enumerate}
        \item compute the \textbf{closed loop transition law} $P^\pi$,
        \item compute the \textbf{state-occupancy measure} from the occupancy measure, this is easy as it has a matrix-geometric series form:
        \[  \mu^\pi_S = (1-\gamma) \lim_{T \rightarrow + \infty} \sum_{t=0}^T  (\gamma P^\pi)^t \cdot \bm{\nu} = (1-\gamma) (I - \gamma P^\pi)^{-1} \bm{\nu}  \]
        \item finally one can compute the \textbf{occupancy measure} by
        \[ \mu(s,a) = \pi(a|s)\mu_S(s).\]
    \end{enumerate}
    Similarly, with knowledge of the occupancy mapping $\bm{\mu}$ one can recover a policy $\pi$ as follows:
    \begin{align*}
        \pi^{\bm{\mu}}(a|s)= \begin{cases}
            \frac{\mu(s,a)}{\sum_a\mu(s,a)}, &  \sum_a \mu(s,a) > 0\\
            \frac{1}{m}, & \text{otherwise}.
        \end{cases}
    \end{align*}
    Notice that the mapping is ill-defined for unvisited states, but this does not matter changes in policy on unvisited states have no impact on the discounted reward.
\end{observation}

For convenience we define two maps that allow us to go from occupancy map to policies and from policies to occupancy maps. Here we state and prove a useful lemma which will be critical to proving the global convergence of CIRL algorithms.

\begin{lemma}[Occupancy measure is Lipschitz with respect to the to the policy]
    \label{lemma:mu_is_lipschitz_with_pi} More specifically, the occupancy measures $\bm{\mu}^{\pi}$ and $\bm{\mu}^{\bar{\pi}}$ respectively induced by policies $\bm{\pi}_1$ and $\bm{\pi}_2$ on identical MDPs (with the same state and action sets $S$ and $A$ and the same \textit{probabilistic markovian transition law} $P$, rewards can be different) satisfy the following inequality:
    \begin{align*}
        \| \bm{\mu}^{\pi} - \bm{\mu}^{\bar{\pi}} \|_2 
        \leq 
        B_\mu \| \bm{\pi} - \bar{\bm{\pi}} \|_2,
    \end{align*}
    where $B_\mu = \frac{1}{1-\gamma}$ and $\|\cdot\|_2$ denotes the l2 norm.
\end{lemma}
\begin{proof}
    We start by expanding the expression from the lefthand side of the inequality we are trying to prove: 
    \begin{align*}
        \| \bm{\mu}^{\pi} - \bm{\mu}^{\bar{\pi}} \|_2 
        &= \sqrt{
            \sum_{s,a\in S\times A} \Bigl(
                \mu^{\pi} (s,a) - \mu^{\bar{\pi}} (s,a)
            \Bigr)^2
        } \\
        & \stackrel{(i)}{=}  \sqrt{
            \sum_{s,a\in S\times A} \Bigl(
                \mu^{\pi} (s) \pi(a|s) - \mu^{\bar{\pi}} (s) \bar{\pi}(a|s)
            \Bigr)^2
        } \\
        & \stackrel{(ii)}{=}  \sqrt{
            \sum_{s,a\in S\times A} \Bigl(
                \mu^{\pi} (s) \big( \pi(a|s) - \bar{\pi}(a|s) \big)
                + \bar{\pi}(a|s) \big( \mu^{\pi} (s) - \mu^{\bar{\pi}} (s)  \big)
            \Bigr)^2
        } \\
        & \stackrel{(iii)}{\leq}  \overbrace{\sqrt{
            \sum_{s,a\in S\times A} \Bigl(
                \mu^{\pi} (s) \big( \pi(a|s) - \bar{\pi}(a|s) \big)
            \Bigr)^2
        } }^{\leq  \| \bm{\pi} - \bar{\bm{\pi}} \|_2 } \\&
        + \overbrace{ \sqrt{
            \sum_{s,a\in S\times A} \Bigl(
                \bar{\pi}(a|s) 
                \big( \mu^{\pi} (s) - \mu^{\bar{\pi}} (s)  \big)
            \Bigr)^2
        } }^{\leq \| \bm{\mu}_s - \bar{\bm{\mu}}_s \|_2}\\
        & \stackrel{(iv)}{\leq} 
        \| \bm{\pi} - \bar{\bm{\pi}} \|_2 +
         \| \bm{\mu}_s - \bar{\bm{\mu}}_s \|_2.
    \end{align*}
    Where in $(i)$ we just plug in the defintion of the state-occupancy measure (def \ref{def:state_occupancy_measure}), in $(ii)$ we just add $0 = \mu^{\pi}(s)\bar{\pi}(a|s) - \mu^{\pi}(s)\bar{\pi}(a|s)$ and rearrang. Next, we just use a triangle inequality $(iii)$ and observing that both sides are upper bounded by l2 norms we are done with the first step. \\

    We will now be concerned with bounding $\| \bm{\mu}_s - \bar{\bm{\mu}}_s \|_2$ by $\| \bm{\pi} - \bar{\bm{\pi}} \|_2$ (mutliplied by some constant term). \\

    To do so we first show a useful result on the spectral norm of the difference between the inverse of two matrices:
    \begin{align*}
        \|A^{-1} + B^{-1}\| 
        & \stackrel{(i)}{=} 
        \| A^{-1} (A+B) B^{-1} \| \\ 
        & \stackrel{(ii)}{\leq}  
        \| A^{-1} \| \cdot \| (A+B) \| \cdot \| B^{-1} \| \\
        & \stackrel{(iii)}{=}  
        \frac{\| (A+B) \|}{\sigma_\text{min}(A)\cdot \sigma_\text{min}(B)}. \tag{l}
    \end{align*}

    Where $(i)$ holds by the equality $A^{-1} + B^{-1} = A^{-1} (A+B) B^{-1}$ which holds for any two invertible matrices (a proof can be found in the solution handbook to \cite{searle82}), $(ii)$ holds by submultiplicativity of the spectral norm and $(iii)$ uses the definition of the spectal norm ($\sigma_\text{min}(A)$ denotes the minimum eigenvalue of the matrix $A$). \\

    We now get back to bounding $\| \bm{\mu}_s - \bar{\bm{\mu}}_s \|_2$:
    \begin{align*}
        \| \bm{\mu}_s - \bar{\bm{\mu}}_s \|_2 
        & \stackrel{(i)}{=} 
        (1-\gamma) \Bigg\| \Big[ 
            \big(I-\gamma P^{\pi}\big)^{-1} 
            - \big(I-\gamma P^{\bar{\pi}}\big)^{-1} 
            \Big] \bm{\nu} \Bigg\|_2 \\
        & \stackrel{(ii)}{\leq} 
        (1-\gamma) \Bigg\| \Big[ 
            \big(I-\gamma P^{\pi}\big)^{-1} 
            - \big(I-\gamma P^{\bar{\pi}}\big)^{-1} 
            \Big] \Bigg\| \cdot \| \bm{\nu} \|_2 \\
        & \stackrel{(iii)}{\leq} 
        (1-\gamma) \Bigg\| \Big[ 
            \big(I-\gamma P^{\pi}\big)^{-1} 
            - \big(I-\gamma P^{\bar{\pi}}\big)^{-1} 
            \Big] \Bigg\| \\
        & \stackrel{(iv)}{\leq} 
        (1-\gamma)^{-1} \Bigg\| \Big[ 
            \big(I-\gamma P^{\pi}\big)
            - \big(I-\gamma P^{\bar{\pi}}\big)
            \Big] \Bigg\| \\
        & = \frac{\gamma}{1-\gamma}
        \big\| 
            P^{\pi} -P^{\bar{\pi}}
        \big\|. 
    \end{align*}
    Where in $(i)$ we just use the closed form computation of the state-occupancy measure from the policy (as shown in obs \ref{obs:policy_occupancy_bijection}), in $(ii)$ we just use the definition of the spectral norm with pulls out the $\|\bm{\nu}\|_2$ term, which we know is smaller or equal to $1$ (since it is the l2 norm of a probability distribution) which gives us inequality $(iii)$. Now plugging in the result from $(l)$ and using that the smallest eigenvalue of $\big(I-\gamma P^{\pi}\big)^{-1}$ is greater or equal to $1-\gamma$ we get inequality $(iv)$ which simplifies into the last line.\\

    Now we just need to bound $ \big\|P^{\pi} -P^{\bar{\pi}} \big\|$ which we do as follows:
    \begin{align*}
        \big\| 
            P^{\pi} -P^{\bar{\pi}}
        \big\|
        & \stackrel{(i)}{\leq} 
        \big\| 
            P^{\pi} -P^{\bar{\pi}}
        \big\|_F\\
        & \stackrel{(ii)}{=} 
        \sqrt{
            \sum_{s,s' \in S \times S} 
            \big(
                P^{\pi}(s'|s) - P^{\bar{\pi}}(s'|s)
            \big)^2
        } \\
        & \stackrel{(iii)}{=} 
        \sqrt{
            \sum_{s,s' \in S \times S} 
            \Big(
                \sum_{a\in A}P(s'|s,a) (\pi(a|s)-\bar{\pi}(a|s))
            \Big)^2
        } \\
        & =
        \sqrt{
            \sum_{s,a,s' \in S \times A \times S} 
            P(s'|s,a)^2 
            (\pi(a|s)-\bar{\pi}(a|s))^2
        } \\
        & =
        \sqrt{
            \sum_{s,a \in S \times A} 
            \Big( \sum_{s' \in S} P(s'|s,a)^2  \Big)
            (\pi(a|s)-\bar{\pi}(a|s))^2
        } \\
        & \leq
        \sqrt{
            \sum_{s,s' \in S \times S} 
            (\pi(a|s)-\bar{\pi}(a|s))^2
        } = \| \bm{\pi} - \bar{\bm{\pi}} \|_2.
    \end{align*}
    Where $(i)$ comes from the fact that the Frobenius norm upper bounds the spectral norm, $(ii)$ is by the definition of the Frobenius norm, and $(iii)$ just plugs in the definition of the closed loop transition law (def \ref{def:closed_loop_transition_law}) from there we can just rearange and isolate the $\big( \sum_{s' \in S} P(s'|s,a)^2 \big)$ term, which since $P(\cdot,s,a) \in \Delta_S$ we know is less than $1$. From there we just observe that we have gotten to the definition of the $l2$ norm. \\

    Putting everything back together we have:
    \begin{align*}
        \| \bm{\mu}^{\pi} - \bm{\mu}^{\bar{\pi}} \|_2 
        &\leq 
        \| \bm{\pi} - \bar{\bm{\pi}} \|_2 +
        \| \bm{\mu}_s - \bar{\bm{\mu}}_s \|_2 \\
        &\leq 
        \| \bm{\pi} - \bar{\bm{\pi}} \|_2 +
        \frac{\gamma}{1-\gamma}
        \big\| 
            P^{\pi} -P^{\bar{\pi}}
        \big\| \\ 
        &\leq 
        \| \bm{\pi} - \bar{\bm{\pi}} \|_2 +
        \frac{\gamma}{1-\gamma}
        \| \bm{\pi} - \bar{\bm{\pi}} \|_2 \\
        & = 
        \frac{1}{1-\gamma}
        \| \bm{\pi} - \bar{\bm{\pi}} \|_2
    \end{align*}
\end{proof}

\begin{definition}
    The $\text{OM}:\Pi \rightarrow \mathcal{M}$ returns the occupancy measure $\bm{\mu}=\text{OM}(\bm{\pi})$ associated with a given policy $\bm{\pi}$.
\end{definition}
\begin{definition}
    The $\text{PO}: \mathcal{M} \rightarrow \Pi$ returns a policy $\bm{\pi}=\text{PO}(\bm{\mu})$ that induces the occupancy measure $\bm{\mu}$.
\end{definition}

We expect that the policy for unvisited state is always uniform ($\pi(a|s_\text{univisted})=1/m~\forall a \in A$), the following property is satisfied
\begin{align*}
    (\text{OM} \circ \text{PO}) = \text{Id}\\
    (\text{PO} \circ \text{OM}) = \text{Id}\\
\end{align*}
where $\text{Id}$ is the identity operator.

\subsubsection{Formulating solving MDPs as optimization problems} \label{sec:opt_sol_mdp}

The goal of an MDP can be stated as follows:
\begin{align}
    \max_{\bm{\pi} \in \Pi} &J(\bm{\pi},\bm{r})
    \label{eq:pb_direct_policy_opt}
\end{align}
i.e. find the policy that gives maximum expected discounted reward in the MDP $\textit{M}$ under initial state distribution $\nu$. Solving the problem \ref{eq:pb_direct_policy_opt} is known as \textit{direct policy optimization}. It is a \textbf{non-concave} optimization problem but we can find algorithms that converge globally on it \cite{Agarwal2020}. Alternatively because of observation \ref{obs:policy_occupancy_bijection} (policy-occupancy measure bijection), one can also formulate solving \ref{eq:pb_direct_policy_opt} in terms of the occupancy measure as follows:
\begin{equation}
    \begin{aligned}
        \max_{\bm{\mu}} & ~~ \langle \bm{r}, \bm{\bm{\mu}} \rangle - \tilde{\Omega}(\bm{\mu})\\
        \text{s.t.} & ~~  (E-\gamma P)^T  \bm{\mu} = (1-\gamma) \bm{\nu},
    \end{aligned}
    \label{eq:pb_occupancy_opt}
\end{equation}
where the equality constraints $ (E-\gamma P)^T  \bm{\mu} = (1-\gamma) \bm{\nu}$ (the \textbf{Bellman Flow Constraints}) are equivalent to restricting $\bm{\mu}$ to the occupancy set $\mathcal{M}$ (see definition \ref{def:occupancy_set}). Note that as the problem (\ref{eq:pb_occupancy_opt}) has a concave cost-function with a set of linear equality constraints, it is a concave program. Observe that in the special case where $\tilde{\Omega}(\bm{\mu})=0$ (\ref{eq:pb_occupancy_opt}) becomes a Linear Program that can be solved in poly-time. The problem (\ref{eq:pb_occupancy_opt}), naturally relaxes to a an associated, unconstrained Lagrangian form:

\begin{equation}
    \max_{\bm{\mu}} \min_{\bm{\lambda} \geq 0 }~~ 
    \langle \bm{r}, \bm{\bm{\mu}} \rangle
    - \tilde{\Omega}(\bm{\mu}) 
    + \langle \bm{\lambda}, \big((E-\gamma P)^T  \bm{\mu} - (1-\gamma) \bm{\nu} \big)\rangle,
    \label{eq:pb_occupancy_lagrangian_opt}
\end{equation}
in which case the problem becomes a \textit{saddle-point problem}. In the unregularized form ($\tilde{\Omega}(\bm{\mu})=0$) the problem is \textit{bilinear} w.r.t the lagrangian multiplier $\bm{\lambda}$ and the occupancy measure (primal variable) $\bm{\mu}$, else it is concave-linear. We sometimes use notation $J(\bm{\mu},\bm{r}) = J(\text{OM}(\bm{\pi}),\bm{r})$ and $K(\bm{\mu},) = K(\text{OM}(\bm{\pi}))$.