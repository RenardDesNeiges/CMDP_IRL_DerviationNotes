
\subsection{Unconstrained Markov Decision Processes} \label{sec:unconstrained_mds}
\begin{definition}
    (\textbf{Markov Decision Process})
    We define a \textbf{Markov Decision Process} (MDP) as the following $6$-tuple: 
    \begin{align*}
        \textestimated{M} = (S,A,P,\bm{\nu},\bm{r},\gamma),
    \end{align*}
    where $S = \{s_1,s_2,...,s_n\}$ is a discrete set of \textbf{states} of size $|S| = n$, $A = \{a_1,a_2,...,a_m\}$ a discrete set of \textbf{actions} of size $|A|=m$, is a probabilistic markovian transition law $P:S\times A \rightarrow \Delta_{S}$ that we can represent as a $\mathbb{R}^{nm\times n}$ matrix (where each column represents in the distribution $\in \Delta_S$), $\nu\in \Delta_s$ is the \textbf{initial distribution} of states at the start of the process, $r:S \times A\rightarrow \mathbb{R}$ is a reward function that we can conveniently represent as a vector $\bm{r}\in\mathbb{R}^{nm}$ since $S$ and $A$ are discrete, finally $\gamma\in(0,1)$ is our \textbf{discount factor}. \\
\end{definition}


Unless specified otherwise it is expected that Markov Decision Process run for an infinite time, each (infinite-time) sequence of move on the MDP is called a \textit{trajectory} and is denoted:
\begin{align*}
    \tau = \{s_1,a_1,s_2,a_2,...\},
\end{align*}
\noindent
where the transition from one state to another is governed by the markovian transition law: 
\begin{align*}
    s_{t+1} \sim P(\cdot|s_t,a_t).
\end{align*} 
The MDP, doesn't specify how actions are selected, in practice, this is done by choosing a \textit{policy function} $\pi:S\rightarrow \Delta_A$, which we use to pick the action $a_t$:
\begin{align*}
    a_t \sim \pi(s_t) \in \Delta_A.
\end{align*}

\begin{definition}
    (\textbf{Policy})
    A \textbf{policy} is a function $\pi:S \rightarrow \Delta_A$ that maps each discrete state to a distribution over actions of an MDP $\textit{M}$. Because we consider discrete state and action sets, we can represent policies as matrices $\pi\in \Pi \in \mathbb{R}^{n \times m}$. Where $\Pi$ denotes the set of \textbf{valid policies}, i.e. : 
    \begin{align*}
        \Pi := \Big\{ \pi = [\pi_{s_1},\pi_{s_2},...,\pi_{s_n}], \pi_{s_i}\in \Delta_A \forall s_i \in S \Big\} .
    \end{align*}
\end{definition}
\noindent
Under policy $\pi$ one can "\textit{close the loop}" and go from a \textit{MDP} to a \textit{markov chain}.

\begin{definition}
    (\textbf{Closed-Loop Transition Law})
    We define the \textbf{closed-loop transition law} $P^\pi :S \rightarrow \Delta_S$ as a function that gives the distribution over the next state as a function of the current state, this distribution is trivially given by: 
    \begin{align*}
        P(s'|s) = \sum_a P(s'|a,s) \cdot \pi(a|s).
    \end{align*}
    \noindent
    It can be represented as a square matrix $P^\pi \in \mathbb{R}^{n \times n}$, where each $i$-th column gives the distribution of states $s'$ associated with the transition from the $i$-th state.
\end{definition}

\subsubsection{Discounted Reward, Value, Quality and Objective Functions}

At each transition the agent receives reward $r_t = r(s_t,a_t)$. We want to quantify the general performance of our agents with respect to that reward function, across complete trajectories. This motivates the definition a series of functions which are used to quantify the performance of a given policy $\pi$ on the MDP. 

\begin{definition}
    (\textbf{Discounted Reward})
    Given a policy $\pi$, and a convex \textbf{policy regularizer function} $\Omega: \Pi \rightarrow \mathbb{R}$, one can compute the \textit{discounted reward} $R(\tau)$ as follows:
    \begin{align*}
        R^{\Omega(\pi)}(\tau) = (1-\gamma) \lim_{T \rightarrow + \infty}\sum_{t=0}^{T} \gamma^t \Bigl( r(s_t,a_t) - \Omega(\pi(\cdot|s_t)) \Bigr) ;~ s_t,a_t \in \tau.
    \end{align*}
    Note that assuming that the regularized rewards ($r(s_t,a_t)- \Omega(\pi(\cdot|s_t))$) are upper-bounded by some constant $C\in \mathbb{R}$, this limit is guaranteed to exist as:
    \begin{align*}
        R(\tau) = (1-\gamma) \lim_{T \rightarrow + \infty}\sum_{t=0}^{T} \gamma^t (r(s_t,a_t)- \Omega(\pi(\cdot|s_t))) \leq (1-\gamma) \lim_{T \rightarrow + \infty}\sum_{t=0}^{T} \gamma^t C = C.
    \end{align*}
\end{definition}
\noindent
Since the process is stochastic it makes more sense to think in terms of \textit{expected discounted reward}, we define the \textit{objective function} $J^\pi$, the \textit{value function} $V^\pi(s)$ and the \textit{quality function} $Q^\pi(s,a)$ (which are all associated with a policy $\pi$).

\begin{definition}
    (\textbf{Value Function})
    For a given policy $\pi:S\rightarrow \Delta A$, a convex \textbf{policy regularizer function} $\Omega: \Pi \rightarrow \mathbb{R}$ and under the assumption that the agent starts at some state $s_0$, we define the \textbf{value function} $V^\pi:S\rightarrow \mathbb{R}$ as:
    \begin{align*}
        V^\pi(s) &= \mathbb{E}\Big[ R(\tau) \Big| s_0  = s, \pi \Big] \\
         &= \mathbb{E}\Bigg[ (1-\gamma) \sum_{t=0}^{+\infty} \gamma^t \Bigl(  r(s_t,a_t) - \Omega(\pi(\cdot|s_t))  \Bigr) \Big| s_0  = s, \pi \Bigg].
    \end{align*}
\end{definition}

\begin{definition}
    (\textbf{Quality Function})
    For a given policy $\pi:S\rightarrow \Delta A$, a convex \textbf{policy regularizer function} $\Omega: \Pi \rightarrow \mathbb{R}$ and under the assumption that the agent starts at some state $s_0$ with first action $a_0$, we define the \textbf{quality function} $Q^\pi:S\times A \rightarrow \mathbb{R}$ as:
    \begin{align*}
        Q^\pi(s,a) &= \mathbb{E}\Big[ R(\tau) \Big| s_0  = s, a_0  = a, \pi \Big] \\
        \\ &= \mathbb{E}\Bigg[ (1-\gamma) \sum_{t=0}^{+\infty} \gamma^t \Bigl( r(s_t,a_t) - \Omega(\pi(\cdot|s_t))  \Bigr)  \Big| s_0  = s, a_0 = a, \pi \Bigg].
    \end{align*}
\end{definition}

\begin{definition}
    (\textbf{Objective Function})
    For a given policy $\pi:S\rightarrow \Delta A$, convex policy regularizer function $\Omega: \Pi \rightarrow \mathbb{R}$ and and under an initial state distribution $\bm{\nu} \in \Delta_S$ we define the \textbf{objective function} as:
    \begin{align*}
        J^\nu(\pi) = \mathbb{E}\Big[ R(\tau) \Big| s_0 \sim \nu, \pi \Big] =  \mathbb{E}\Big[ V^\pi(s) \Big| s \sim \nu\Big].
    \end{align*}
    Since we most often use the \textbf{objective function} for optimization in policy space, we consider that $J^\nu:\pi \rightarrow \mathbb{R}$ is a function of the policy $\pi$.
\end{definition}

\subsubsection{The Occupancy Measure}

We now discuss a descriptor of the distribution of an agent's position across all states induced by some policy $\pi$: the \textit{occupancy measure} is defined as follows.

\begin{definition}
    (\textbf{Occupancy Measure}) 
    The (state-action) \textbf{occupancy measure} $\bm{\mu}^\pi \in \Delta_{S \times A}$ is defined for a given state-action pair as follows:
    \begin{align*}
        \mu^\pi(s,a) &= (1-\gamma) \lim_{T \rightarrow + \infty} \sum_{t=0}^T \gamma^t \mathbb{P}_\nu^\pi(s_t=s,a_t=a) 
        \\
        &= (1-\gamma) \mathbb{E} \Big[ \sum_{t=0}^{+\infty} \gamma^t \mathbf{1}(s_t=s,a_t=a) \Big| s_0 \sim \nu, \pi\Big],
    \end{align*}
    since our state and action spaces are discrete we can represent $\bm{\mu}$ as a vector in $\mathbb{R}^{nm}$. 
\end{definition}

\begin{definition}
    \label{def:occupancy_set}
    (\textbf{Occupancy Set})
    The \textbf{set of valid occupancy measure} (or occupancy set) is given by:
    \begin{align*}
        \mathcal{M} := \Bigl\{ \mu \in \mathbb{R}_+^{nm} : (E-\gamma P)^T \bm{\mu} = (1-\gamma) \bm{\nu} \Bigr\}.
    \end{align*}
    The equality that all points $\bm{\mu}$ in the set must satisfy are known as \textbf{Bellman Flow Constraints}.
\end{definition}

\begin{definition}
    (\textbf{State-Occupancy Measure}) 
    Similarly to the (state-action) occupancy measure, one can define a \textbf{state-occupancy measure} $\bm{\mu}^\pi_S \in \Delta_{S}$ is defined for a given state as follows:
    \begin{align*}
        \mu^\pi_S(s) &= (1-\gamma) \lim_{T \rightarrow + \infty} \sum_{t=0}^T \gamma^t \mathbb{P}_\nu^\pi(s_t=s) \\
        &= (1-\gamma)  \mathbb{E} \Big[ \sum_{t=0}^{+\infty} \gamma^t \mathbf{1}(s_t=s) \Big| s_0 \sim \nu, \pi\Big],
    \end{align*}
    since our state and action spaces are discrete we can represent $\bm{\mu}_S$ as a vector in $\mathbb{R}^{n}$.
\end{definition}
\noindent
Furthermore, when an state action pair $s,a$ has occupancy measure $\mu^\pi(s,a)=0$ (under policy $\pi$), we say that it is \textit{unvisited}.

\begin{observation}
    \label{obs:mu_pi_objective}
    The occupancy measure provides us with a very concise notation for writing out quantities such as the \textbf{objective function}, which can be represented in a simple scalar product form:
    \begin{align*}
        J^\nu(\pi) = \bm{r}^T \bm{\mu}^\pi - f(\bm{\mu}^\pi).
    \end{align*}
    Which can be easily verified as follows:
    \begin{align*}
        J^\nu(\pi) &= (1-\gamma)  \mathbb{E}\Big[ R(\tau) \Big| s_0 \sim \nu, \pi \Big] \\
                &=  (1-\gamma) \mathbb{E}\Bigg[ \sum_{t=0}^{+\infty} \gamma^t (r(s,a) - \Omega(\pi(\cdot|s))) \Big| s_0 \sim \nu, \pi \Bigg]\\
                &= (1-\gamma)  \mathbb{E}\Bigg[ \sum_{s,a} \sum_{t=0}^{+\infty} \gamma^t  r(s,a) \textbf{1}(s,a) \Big| s_0 \sim \nu, \pi \Bigg] \\& -  (1-\gamma)  \mathbb{E}\Bigg[ \sum_{s} \sum_{t=0}^{+\infty} \gamma^t \Omega(\pi(\cdot|s))) \textbf{1}(s) \Big| s_0 \sim \nu, \pi \Bigg]   \\
                &= \sum_{s,a} r(s,a)\overbrace{ (1-\gamma) \mathbb{E}\Bigg[  \sum_{t=0}^{+\infty}\gamma^t  \textbf{1}(s,a) \Big| s_0 \sim \nu, \pi \Bigg]}^{=\bm{\mu}^\pi(s,a)}\\
                &- \sum_{s} \Omega(\pi(\cdot|s)) \overbrace{ (1-\gamma) \mathbb{E}\Bigg[  \sum_{t=0}^{+\infty} \gamma^t \textbf{1}(s) \Big| s_0 \sim \nu, \pi \Bigg]}^{=\bm{\mu}_S^\pi(s)}\\
                &= \sum_{s,a} r(s,a) \bm{\mu}^\pi(s,a) - \overbrace{\sum_{s} \Omega(\pi(\cdot|s)) \bm{\mu}_S^\pi(s) }^{:=\tilde{\Omega}(\bm{\mu})}\\
                &=  \bm{r}^T \bm{\bm{\mu}^\pi} - \tilde{\Omega}(\bm{\bm{\mu}}^\pi).
    \end{align*}
    This is quite important as it shows that the objective function is \textbf{concave} w.r.t the occupancy measure (which opens the door for optimization approaches), moreover in the special case where $\Omega(\pi(\cdot|s)) = 0, ~\forall s \in S $ the objective is \textbf{linear} w.r.t the occupancy measure (which opens the door for linear programming approaches). \\
    For convenience we interchangeably write the objective as a function of the policy $\pi$ and of the occupancy measure $\bm{\mu}$:
    \begin{align*}
        J^\nu(\pi) =  J^\nu(\bm{\mu}).
    \end{align*}
\end{observation}

\begin{observation}
    \label{obs:policy_occupancy_bijection}
    There exists a (almost) well-defined bijection between any policy $\pi$ and it's associated occupancy measure $\bm{\mu}^\pi$. With knowledge of $P$ and $\pi$, one can compute the occupancy measure from the bellman flow constraint as follows:
    \begin{enumerate}
        \item compute the \textbf{closed loop transition law} $P^\pi$,
        \item compute the \textbf{state-occupancy measure} from the occupancy measure, this is easy as it has a matrix-geometric series form:
        \[  \mu^\pi_S = (1-\gamma) \lim_{T \rightarrow + \infty} \sum_{t=0}^T  (\gamma P^\pi)^t = (1-\gamma) (I - \gamma P^\pi)^{-1} \]
        \item finally one can compute the \textbf{occupancy measure} through
        \[ \mu(s,a) = \pi(a|s)\mu_S(s).\]
    \end{enumerate}
    Similarly, with knowledge of the occupancy mapping $\bm{\mu}$ one can recover a policy $\pi$ as follows:
    \begin{align*}
        \pi^{\bm{\mu}}(a|s)= \begin{cases}
            \frac{\mu(s,a)}{\sum_a\mu(s,a)}, &  \sum_a \mu(s,a) > 0\\
            \frac{1}{m}, & \text{otherwise}.
        \end{cases}
    \end{align*}
    Notice that the mapping is ill-defined for unvisited states, but this does not matter changes in policy on unvisited states have no impact on the discounted reward.
\end{observation}

\subsubsection{Formulating the search of optimal policies as optimization problems} \label{sec:opt_sol_mdp}

The goal of an MDP can be stated as follows:
\begin{align}
    \max_{\pi \in \Pi} J^\nu(\pi),
    \label{eq:pb_direct_policy_opt}
\end{align}
i.e. find the policy that gives maximum expected discounted reward in the MDP $\textit{M}$ under initial state distribution $\nu$. Solving the problem \ref{eq:pb_direct_policy_opt} is known as \textit{direct policy optimization}. It is a \textbf{non-concave} optimization problem but we can find algorithms that converge globally on it. Alternatively because of observation \ref{obs:policy_occupancy_bijection} (policy-occupancy measure bijection), one can also formulate solving \ref{eq:pb_direct_policy_opt} in terms of the occupancy measure as follows:
\begin{equation}
    \begin{aligned}
        \max_{\bm{\mu}} & ~~ \bm{r}^T\bm{\bm{\mu}} - \tilde{\Omega}(\bm{\mu})\\
        \text{s.t.} & ~~  (E-\gamma P)^T  \bm{\mu} = (1-\gamma) \bm{\nu},
    \end{aligned}
    \label{eq:pb_occupancy_opt}
\end{equation}
where the equality constraints $ (E-\gamma P)^T  \bm{\mu} = (1-\gamma) \bm{\nu}$ (the \textbf{Bellman Flow Constraints}) are equivalent to restricting $\bm{\mu}$ to the occupancy set $\mathcal{M}$ (see definition \ref{def:occupancy_set}). Note that as the problem (\ref{eq:pb_occupancy_opt}) has a concave cost-function with a set of linear equality constraints, it is a concave program. Observe that in the special case where $\tilde{\Omega}(\bm{\mu})=0$ (\ref{eq:pb_occupancy_opt}) becomes a Linear Program that can be solved in poly-time. The problem (\ref{eq:pb_occupancy_opt}), naturally relaxes to a an associated, unconstrained Lagrangian form:

\begin{equation}
    \max_{\bm{\mu}} \min_{\bm{\lambda} \geq 0 }~~ \bm{r}^T\bm{\bm{\mu}} - \tilde{\Omega}(\bm{\mu}) + \bm{\lambda}^T \big((E-\gamma P)^T  \bm{\mu} - (1-\gamma) \bm{\nu} \big),
    \label{eq:pb_occupancy_lagrangian_opt}
\end{equation}
in which case the problem becomes a \textit{saddle-point problem}. In the unregularized form ($\tilde{\Omega}(\bm{\mu})=0$) the problem is \textit{bilinear} w.r.t the lagrangian multiplier $\bm{\lambda}$ and the occupancy measure (primal variable) $\bm{\mu}$, else it is concave-linear.