We will now consider the inverse problem to the MDP and CMDP problems that we discussed before, \textbf{Constrained Inverse Reinforcement Learning} (IRL). In the IRL setting (\textit{on a known MDP}) we are given an \textit{MDP} or a \textit{CMDP} without its reward:
\begin{align*}
    \textit{CMDP} \setminus \bm{r} = (S,A,P,\bm{\nu},\Psi,\bm{b},\gamma),
\end{align*}
as well as a dataset $\mathcal{D}$ of expert example in the form of trajectories:
\begin{align*}
    \mathcal{D} = \{  \tau_1,\tau_2,...,\tau_N\},
\end{align*}
produced by some expert policy $\bm{\pi}^E$. Assuming the dataset is large enough (which we will do at first), this is equivalent to getting an approximation of the expert's occupancy measure $\bm{\mu}^E$. In the CIRL setting we also generally restrict the reward class $\mathcal{R}$ to some arbitrary convex set in $\mathbb{R}^{nm}$. \\
\begin{definition}
    \label{def:CIRL_solmap}
    (\textbf{CIRL Solution Map})
    The \textbf{goal of CIRL} is to find some mapping $\text{CIRL}_\pi : \Pi \rightarrow \mathcal{R}$ (or alternatively $\text{CIRL}_\mu : \mathcal{M} \rightarrow \mathcal{R}$). That satisfies
    \begin{align*}
        (\text{CRL}_\pi \circ \text{CIRL}_\pi)(\bm{\pi}^E) = \bm{\pi}^E.
    \end{align*}
\end{definition}
In plain english \textbf{we want to find a method for recovering a reward for which the original expert is optimal.}

\begin{assumption}
    \label{assumption:realizability}
    (\textbf{Realizability})
    We assume that the expert policy is optimal w.r.t. some reward $\bm{r}^E \in \mathcal{R}$, i.e. 
    \begin{align*}
        \bm{\mu}^E = \text{CRL}_{\mu}(\bm{r}^E).
    \end{align*}
\end{assumption}
\noindent
The goal of the IRL problem is to recover a reward $\hat{\bm{r}}$ s.t. the expert $\bm{\mu}^E = \text{CRL}_{\mu}(\hat{\bm{r}})$ (the expert is optimal for that reward).

\begin{proposition}
    \label{proposition:minmaxIRL}
    (\textbf{Min-Max Program to solve IRL})
    If assumption \ref{assumption:realizability} is true, then the rewards optimizing the program:
    \begin{equation}
        \label{eq:minmaxIRL}
        \begin{aligned}
            \min_{\bm{r}\in\mathcal{R}}\max_{\bm{\mu}\in\mathcal{F}} 
            \langle \bm{r}, \bm{\mu}-\bm{\mu}^E \rangle-\tilde{\Omega}(\bm{\mu}),
        \end{aligned}
    \end{equation}
    are exactly the rewards in $\mathcal{R}$ for which $\bm{\mu}^E$ is optimal.
\end{proposition}
\begin{proof}
    (Of proposition \ref{proposition:minmaxIRL}) We can rewrite an equivalent problem to (\ref{eq:minmaxIRL}) as follows:
    \begin{align*}
        \min_{\bm{r}\in\mathcal{R}}\max_{\bm{\mu}\in\mathcal{F}} 
        \langle \bm{r}, \bm{\mu} \rangle 
        - \tilde{\Omega}(\bm{\mu}) 
        - \langle \bm{r}, \bm{\mu}^E \rangle
        + \tilde{\Omega}(\bm{\mu}^E),
    \end{align*}
    where the addition of the $\tilde{\Omega}(\bm{\mu}^E)$ term has no influence on the optimizers, as it is a function of neither of the decision variables. For convenience we write:
    \begin{align*}
        \bm{r}^T\bm{\mu}-\tilde{\Omega}(\bm{\mu}) - \bm{r}^T \bm{\mu}^E + \tilde{\Omega}(\bm{\mu}^E) = L(\bm{r},\bm{\mu}).
    \end{align*}
    For any fixed $\bm{r}\in\mathcal{R}$ it holds that 
    \begin{align*}
        \arg\max_{\bm{\mu}\in\mathcal{F}} 
        \langle \bm{r}, \bm{\mu} \rangle
        -\tilde{\Omega}(\bm{\mu}) 
        - \overbrace{
            \langle \bm{r}, \bm{\mu}^E \rangle 
            + \tilde{\Omega}(\bm{\mu}^E)
        }^\text{constant w.r.t $\bm{\mu}$} = \text{CRL}_\mu(\bm{r}), ~\bm{r}\in \mathcal{R},
    \end{align*}
    i.e. the optimal $\bm{\mu}$ for any fixed $\bm{r} \in \mathcal{R}$ gives the optimal policy. Moreover, for any fixed $\bm{r} \in \mathcal{R}$, we get that:
    \begin{align*}
        \max_{\bm{\mu}\in\mathcal{F}} L(\bm{r},\bm{\mu}).\geq 0, ~\bm{r}\in \mathcal{R},
    \end{align*}
    since:
    \begin{enumerate}
        \item either $\mu^E$ maximizes the value $\langle \bm{r}, \bm{\mu}^E \rangle - \tilde{\Omega}(\bm{\mu}^E)$, in which case, we can set the optimizer $\bm{\mu} = \bm{\mu}^E$ and get $ L(\bm{r},\bm{\mu})=0$,
        \item or  $\mu^E$ does not maximize the value $\langle \bm{r}, \bm{\mu}^E\rangle - \tilde{\Omega}(\bm{\mu}^E)$, in  which case we get $ L(\bm{r},\bm{\mu})>0$.
    \end{enumerate}
    Observe that the lower bound ($ L(\bm{r},\bm{\mu})=0$) is only achieved if $\langle \bm{r}, \bm{\mu}^E\rangle - \tilde{\Omega}(\bm{\mu}^E)= \langle \bm{r}, \bm{\mu} \rangle - \tilde{\Omega}(\bm{\mu})$, i.e. if $\bm{\mu}^E \in \arg \max_{\bm{\mu} \in \mathcal{F}} \langle \bm{r}, \bm{\mu}^E \rangle + \tilde{\Omega}(\bm{\mu}^E)$ which is equivalent to $\bm{\mu}^E \in  \text{CRL}_\mu(\bm{r})$. I.e. the optimal reward $\bm{r}^*$ of the min-max problem is such that the expert $\bm{\mu}^E$ is optimal w.r.t $\bm{r}^*$.
    
\end{proof}

\begin{proposition}[Strong Duality]
    \label{prop:strong_duality}
    Assuming that $\exists \bm{\pi}^E\in\mathcal{F}$, and that $\text{obj}$ is strictly convex. Then dual optimum is attained for some $\bm{\lambda}^* \geq 0$ and problem \ref{eq:minmaxConstrainedBanditIRL} is equivalent to an unconstrained bandit problem with reward $\bm{r}-\Psi \bm{\lambda}^*$. I.e. 
    \begin{align*}
        \text{CRL}_\pi(\bm{r}) = \text{RL}_\pi(\bm{r} - \bm{\Psi} \bm{\lambda}^*).
    \end{align*}
\end{proposition}
\begin{proof}
    Using generic Lagrangian Duality theory, we make two observations:
    \begin{enumerate}
        \item observe that the problem: $\max_{\bm{\mu} \in \mathcal{F}} \langle \bm{r}, \bm{\mu} \rangle - \tilde{\Omega}(\bm{\pi})$, is a \textbf{strictly convex optimization problem} (for a proof of the convexity of $\tilde{\Omega}$, assuming that $\Omega$ is convex refer to \cite{Schlaginhaufen2023}),
        \item the primal optimum $\bm{\pi}^*$ is finite as the feasible set $\mathcal{F}$ is bounded and the objective is upper-bounded (since $\Omega$ is strictly convex).
    \end{enumerate}
    From Slater's condition it follows that strong duality holds.
\end{proof}

\begin{definition}[Diminished Reward]
    \label{def:diminished_reward}
    We call diminished reward the function $\tilde{r}:S \times A\rightarrow \mathcal{R}$ defined as:
    \begin{align*}
        \tilde{\bm{r}} = \bm{r} - \bm{\Psi} \bm{\lambda}, \\
        \tilde{{r}}(s,a) = \bm{r}(s,a) - \langle \bm{\Psi}(s,a),\bm{\lambda}(s,a)\rangle, \\
    \end{align*}
    this definition provides a shorthand for making use of strong duality (proposition \ref{prop:strong_duality}) in the analysis of CIRL convergence.
\end{definition}